{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chinese graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import codecs\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = codecs.open('./chinesegraph/chinese_words.csv', 'rb', encoding='utf-8')\n",
    "words = [line.strip() for line in f]\\\n",
    "print 'number of words : ', len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for w in words:\n",
    "    chars = chars.union(set( w ))\n",
    "    # Creating a little 'index'\n",
    "print 'number of chars : ', len(chars)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for ch in chars:\n",
    "    G.add_node(ch,{'chinese':ch})\n",
    "\n",
    "print 'number of chars nodes : ', G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,w in enumerate(words):\n",
    "    G.add_node(str(i),{'chinese': w })\n",
    "    for ch in w:\n",
    "        G.add_edge(str(i),ch)\n",
    "\n",
    "print 'number of nodes (chars + words): ', G.number_of_nodes()\n",
    "\n",
    "nx.write_graphml(G,'chinese_graph.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "from random import choice\n",
    "from math import log\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IS_CHINESE_TEST = True\n",
    "\n",
    "if IS_CHINESE_TEST:\n",
    "    G = nx.read_graphml('./data/graphs/chinese_graph.graphml', unicode) # Chinese graph\n",
    "else:\n",
    "    G = nx.read_graphml('graphs/bipartite_graph.graphml', unicode) # Korean language graph    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen = nx.connected_component_subgraphs(G)\n",
    "G = next(gen)\n",
    "output_dir = \"tmpDistancesFull2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sts = nx.bipartite.sets(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = None\n",
    "hanjas = None\n",
    "wordSet = None\n",
    "\n",
    "if len(sts[0]) > 6000: \n",
    "    words = list(sts[0])\n",
    "    wordSet = sts[0]\n",
    "    hanjas = list(sts[1])\n",
    "else: \n",
    "    words = list(sts[1])\n",
    "    wordSet = sts[1]\n",
    "    hanjas = list(sts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordic = {G.node[e]['chinese']:k for k,e in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synonyms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('./data/synonyms/all_chinese_synonyms.csv', 'r', encoding='utf-8') as f:\n",
    "        synonyms = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synDP = []\n",
    "randDP = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = nx.bipartite.biadjacency_matrix(G, hanjas)\n",
    "A = A.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight = False # In case we chose not to do TF-IDF weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minK = 666\n",
    "maxK = 668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U,s,V = sp.sparse.linalg.svds(A,800)\n",
    "Vtr = V.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pr in synonyms:\n",
    "\n",
    "    word = pr.split(',')\n",
    "    c1 = word[0]\n",
    "    c2 = word[1]\n",
    "    \n",
    "    if c1 not in wordic or c2 not in wordic: continue\n",
    "\n",
    "    v1 = Vtr[wordic[c1]]\n",
    "    v2 = Vtr[wordic[c2]]\n",
    "    synDP.append([ np.dot(v1,v2), c1.encode('utf-8'), c2.encode('utf-8') ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in synonyms:\n",
    "    v1 = choice(Vtr)\n",
    "    v2 = choice(Vtr)\n",
    "    randDP.append(np.dot(v1,v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dir = './output'\n",
    "with codecs.open(\"{}/randdist_k={}.csv\".format(output_dir,k), 'w', encoding='utf-8') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows([[r] for r in randDP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(\"{}/syndist_k={}.csv\".format(output_dir,k), 'w') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(synDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
